{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-multilearn\n",
        "!pip install neattext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jqUvVlPJaY4",
        "outputId": "c0e3fe01-df1c-4250-b381-3072b0b54862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-multilearn\n",
            "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n",
            "Collecting neattext\n",
            "  Downloading neattext-0.1.3-py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: neattext\n",
            "Successfully installed neattext-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Мы использовали модель мультиномиального наивного Байеса (Multinomial Naive Bayes) вместе с методом преобразования проблемы Classifier Chain для решения задачи мультиклассовой классификации текста. В качестве предварительной обработки текста мы использовали различные методы из библиотеки Neattext, такие как удаление шума и стоп-слов. Для векторизации текстовых данных в числовой формат мы использовали метод TF-IDF.\n",
        "\n",
        "Перед обучением модели мы преобразовали метки классов в удобный формат и создали столбцы для каждой метки. Далее разделили данные на обучающий и тестовый наборы.\n",
        "\n",
        "Модель мультиномиального наивного Байеса была обучена на обучающем наборе данных с использованием TF-IDF преобразования для векторизации текста. Затем мы делали предсказания на тестовом наборе данных и оценивали производительность модели с помощью F1-меры.\n",
        "'''\n",
        "import pandas as pd\n",
        "import ast\n",
        "import random\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "import neattext as nt\n",
        "import neattext.functions as nfx\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Определяем размер чанка для обработки данных порциями\n",
        "chunksize = 5000\n",
        "# Загружаем обучающий набор данных из CSV файла\n",
        "df = pd.read_csv('drive/MyDrive/train2.csv')\n",
        "\n",
        "# Функция для извлечения меток из строки, представленной в виде списка\n",
        "def extract_labels(label_str):\n",
        "    label_list = ast.literal_eval(str(label_str))\n",
        "    return list(label_list)\n",
        "\n",
        "# Применяем функцию извлечения меток к столбцу 'labels'\n",
        "df['labels'] = df['labels'].apply(extract_labels)\n",
        "\n",
        "# Удаляем шум и стоп-слова из текста\n",
        "df['text'].apply(lambda x: nt.TextFrame(x).noise_scan())\n",
        "\n",
        "df['text'].apply(lambda x: nt.TextExtractor(x).extract_stopwords())\n",
        "\n",
        "df['text'].apply(nfx.remove_stopwords)\n",
        "\n",
        "# Создаем список всех возможных меток\n",
        "all_labels = ['work', 'news', 'sports', 'music', 'movies',\n",
        "              'politics', 'phones', 'self-driving cars',\n",
        "              'family', 'cars', 'climate change', 'languages',\n",
        "              'business', 'health', 'science', 'style', 'opinion',\n",
        "              'economy', 'history', 'technology', 'affair', 'development', 'mobility']\n",
        "\n",
        "for label in all_labels:\n",
        "    df[label] = 0.0\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    for label in row['labels']:\n",
        "        df.at[i, label] = 1.0\n",
        "\n",
        "# Определяем целевые метки\n",
        "y = df[['work', 'news', 'sports', 'music', 'movies',\n",
        "        'politics', 'phones', 'self-driving cars',\n",
        "        'family', 'cars', 'climate change', 'languages',\n",
        "        'business', 'health', 'science', 'style', 'opinion',\n",
        "        'economy', 'history', 'technology', 'affair', 'development', 'mobility']]\n",
        "\n",
        "# Разделяем данные на обучающий и тестовый наборы\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Создаем конвейер для обработки данных и обучения модели\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', ClassifierChain(MultinomialNB())),\n",
        "])\n",
        "\n",
        "# Обучаем конвейер на обучающих данных\n",
        "pipeline.fit(X_train, y_train)\n",
        "# Делаем предсказания на тестовых данных\n",
        "clf_predictions = pipeline.predict(X_test)\n",
        "# Выводим F1-меру\n",
        "print(f1_score(y_test, clf_predictions, average='weighted', zero_division=0))\n",
        "\n",
        "# Функция для преобразования предсказаний в объекты\n",
        "def convert_predictions_to_objects(predictions, labels):\n",
        "    predicted_objects_set = set()\n",
        "\n",
        "    for instance_predictions in predictions:\n",
        "        instance_objects = []\n",
        "\n",
        "        for prediction, label in zip(instance_predictions, labels):\n",
        "            if prediction == 1.0:\n",
        "                instance_objects.append(label)\n",
        "\n",
        "        predicted_objects_set.update(instance_objects)\n",
        "\n",
        "    if len(predicted_objects_set) == 0:\n",
        "        predicted_objects_set.update({random.choice(all_labels)})\n",
        "\n",
        "    return predicted_objects_set\n",
        "\n",
        "# Обработка данных тестового набора и создание предсказаний\n",
        "for chunk in pd.read_csv('drive/MyDrive/test2.csv', chunksize=chunksize):\n",
        "    chunk.text = chunk.text.astype(str)\n",
        "    chunk['text'].apply(nfx.remove_stopwords)\n",
        "    chunk['text'].apply(lambda x: nt.TextFrame(x).noise_scan())\n",
        "    chunk['text'].apply(lambda x: nt.TextExtractor(x).extract_stopwords())\n",
        "    clf_predictions = pipeline.predict(chunk[\"text\"])\n",
        "    new_pred = []\n",
        "\n",
        "    for pred in clf_predictions:\n",
        "        new_pred.append(convert_predictions_to_objects(pred.toarray(), all_labels))\n",
        "\n",
        "    results_df = pd.DataFrame({\n",
        "        '': chunk.index,\n",
        "        'labels': new_pred\n",
        "    })\n",
        "\n",
        "    results_df.to_csv(\"results.csv\", index=False, mode='a',\n",
        "                      header=False)\n",
        "\n",
        "print(results_df.head())"
      ],
      "metadata": {
        "id": "AO4WZi-QJU-r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}